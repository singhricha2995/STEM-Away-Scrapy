{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forum_Classifier_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b6ea00631575417bb3dd4736da1455d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b4ee0ffb5e4417baaa226f922e60206",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b7593d3930247119675d55f4fa2084c",
              "IPY_MODEL_ddf32773b8494b6199929c1c481737a4"
            ]
          }
        },
        "4b4ee0ffb5e4417baaa226f922e60206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b7593d3930247119675d55f4fa2084c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_619c3a115c7f4ea984e4be928d3a2941",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_427c0bd83bd54e7b8b9d7df3f5333bba"
          }
        },
        "ddf32773b8494b6199929c1c481737a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f6bd52752ab24003bee2696ae12cf1d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 207kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2730b9bb538a41daa9b675640ea61d28"
          }
        },
        "619c3a115c7f4ea984e4be928d3a2941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "427c0bd83bd54e7b8b9d7df3f5333bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6bd52752ab24003bee2696ae12cf1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2730b9bb538a41daa9b675640ea61d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a20c954b2ef34262be942e0fb9420073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99d2a18b397b461994c5ac68aa10f956",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d44034f4a574540888422bc1939f3c0",
              "IPY_MODEL_e4983d34d5f948119b5905c015595027"
            ]
          }
        },
        "99d2a18b397b461994c5ac68aa10f956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d44034f4a574540888422bc1939f3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b652646ab7b547fd9c2a5a9d244bbe12",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21d894673baa43259d1ea375f2a6d961"
          }
        },
        "e4983d34d5f948119b5905c015595027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d00f704033284be786a20d8072fd226e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [01:37&lt;00:00, 4.43B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8dcba18b75c48c7be61db92c76170a3"
          }
        },
        "b652646ab7b547fd9c2a5a9d244bbe12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21d894673baa43259d1ea375f2a6d961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d00f704033284be786a20d8072fd226e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8dcba18b75c48c7be61db92c76170a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27b57bde65d34b509b95e913bf22c7ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9d9b60d3a784f41b378c8bd15fd7401",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbc791cb8e3d4f64ab4b4bf30b61f9ca",
              "IPY_MODEL_4b8819c62cd2443ca1d0568e38540e85"
            ]
          }
        },
        "a9d9b60d3a784f41b378c8bd15fd7401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbc791cb8e3d4f64ab4b4bf30b61f9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd88a06580914e2e8b96c350f679e975",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45b415c577d44eb0998131b1f4cb636d"
          }
        },
        "4b8819c62cd2443ca1d0568e38540e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18a8e958a8ba4cf5a08bc353d9ce5491",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:45&lt;00:00, 9.65MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c294e14a64824565b089354f85648d56"
          }
        },
        "dd88a06580914e2e8b96c350f679e975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45b415c577d44eb0998131b1f4cb636d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18a8e958a8ba4cf5a08bc353d9ce5491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c294e14a64824565b089354f85648d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JYsrMAs5in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3d34bfb-cba2-4283-ee23-0e7ce69b0848"
      },
      "source": [
        "!pip install transformers pandas google.colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Collecting google.colab\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/9f/d3ec1275a089ec017f9c91af22ecd1e2fe738254b944e7a1f9528fcfacd0/google-colab-1.0.0.tar.gz (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 32.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Collecting google-auth~=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/80/369a47c28ce7d9be6a6973338133d073864d8efbb62747e414c34a3a5f4f/google_auth-1.4.2-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting ipykernel~=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/c3/76775a650cae2e3d9c033b26153583e61282692d9a3af12a3022d8f0cefa/ipykernel-4.6.1-py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 63.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython~=5.5.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (5.5.0)\n",
            "Requirement already satisfied: notebook~=5.2.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (5.2.2)\n",
            "Requirement already satisfied: six~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (1.12.0)\n",
            "Collecting portpicker~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/49/2c/a75ef568273036aa61319a554164e6031e31708106ea6ca10e17265e1703/portpicker-1.2.0.tar.gz\n",
            "Requirement already satisfied: tornado~=4.5.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (4.5.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (4.1.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel~=4.6.0->google.colab) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel~=4.6.0->google.colab) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (47.3.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (0.8.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (5.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (2.11.2)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (0.8.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth~=1.4.0->google.colab) (0.4.8)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel~=4.6.0->google.colab) (19.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython~=5.5.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.1.5)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook~=5.2.0->google.colab) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook~=5.2.0->google.colab) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
            "Building wheels for collected packages: google.colab, sacremoses, portpicker\n",
            "  Building wheel for google.colab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google.colab: filename=google_colab-1.0.0-py2.py3-none-any.whl size=102289 sha256=5a467c592940b7fb9565fa2baeaad89d6329a4f9cd61aeae46e965fc71db06e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/0d/59/701e300a337b2a2e07b27fe74dbfff0bc56ac58f711566ee67\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4714b31b451c6dc67f609d01e64f2c896f4c9624798f06386e43abfbfc542bae\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for portpicker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for portpicker: filename=portpicker-1.2.0-cp36-none-any.whl size=13369 sha256=c3b52dfdd8e198fd13ade39c797502c284ef89d766bb87162e92af83002c1ec0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/45/47/1e126be9d4605e71f00d6e6fb151611f2f4cb9770b050c7d2d\n",
            "Successfully built google.colab sacremoses portpicker\n",
            "\u001b[31mERROR: tensorboard 2.2.2 has requirement google-auth<2,>=1.6.3, but you'll have google-auth 1.4.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 1.0.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, google-auth, ipykernel, portpicker, google.colab\n",
            "  Found existing installation: google-auth 1.17.2\n",
            "    Uninstalling google-auth-1.17.2:\n",
            "      Successfully uninstalled google-auth-1.17.2\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Found existing installation: portpicker 1.3.1\n",
            "    Uninstalling portpicker-1.3.1:\n",
            "      Successfully uninstalled portpicker-1.3.1\n",
            "Successfully installed google-auth-1.4.2 google.colab ipykernel-4.6.1 portpicker-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "ipykernel",
                  "portpicker"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBi6gMTy4Hp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "96510a60-dc9d-4604-ff5a-b0425139a42d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "spreadsheet_name = ['huel_forum_posts',\n",
        "                    'bnz_forum_posts',\n",
        "                    'airline_forum_updated',\n",
        "                    'quickfile_forum_posts',\n",
        "                    'codecombat_forum_posts',\n",
        "                    'schizophrenia_forum_posts',\n",
        "                    'folksy_forum_posts']\n",
        "\n",
        "def loadgsheet(spreadsheet_name):\n",
        "  spreadsheet=pd.read_excel(f'gdrive/My Drive/bert_data/{spreadsheet_name}.xlsx')\n",
        "  spreadsheet.columns = ['post_text', 'post_id', 'user_id', 'username', 'reply_to_post_num', 'topic_id', 'post_num',\n",
        "             'reply_count', 'created_at', 'updated_at', 'num_reads', 'topic_slug', 'forum_name']\n",
        "  return spreadsheet[:2000]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZos8TXY_xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97be3c8e-db5a-4e90-a1d2-61a441a78919"
      },
      "source": [
        "df_all = pd.DataFrame()\n",
        "for sheet_name in spreadsheet_name:\n",
        "  print(f'Joining {sheet_name}')\n",
        "  df = pd.read_excel(f'gdrive/My Drive/bert_data/{sheet_name}.xlsx')\n",
        "  df.columns = ['post_text', 'post_id', 'user_id', 'username', 'reply_to_post_num', 'topic_id', 'post_num',\n",
        "             'reply_count', 'created_at', 'updated_at', 'num_reads', 'topic_slug', 'forum_name']\n",
        "  print(df.head(2))\n",
        "  df_all = pd.concat([df_all, df]).dropna(axis=0)\n",
        "df_all.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joining huel_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       post_text  ...  forum_name\n",
            "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)   ...  huel      \n",
            "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  huel      \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining bnz_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                post_text  ...  forum_name\n",
            "0  Today is **‘Safer Internet Day’** , which is celebrated globally in February each year. It’s a day dedicated to promoting a safer online world. Netsafe are the official organising committee for this in New Zealand, and BNZ are proud to support them to encourage conversations about what a safer internet could look like. At BNZ, we think it’s an important day to recognise given that if you’re not staying safe online, you’re more likely to become a target of financial crime.     Netsafe have shared some great tips and resources, including online safety conversation starters for kids (under ten year olds), tweens (10-13 year olds) and teens (14-18 year olds). They recommend having regular chats with your young ones to educate them about online safety, and ultimately help minimise the damage if things do go wrong online. The types of questions they suggest asking your children are:  * What are some things you shouldn’t share online?  * What is the difference between public and private posts?  * Do you know what privacy settings are and what they are used for?    On top of this, Netsafe have a ‘Parents Toolkit’, as well as guides for checking privacy settings for some of the major social media channels – Facebook, Twitter, Instagram and Snapchat.     To read these resources in detail, and even print them off to take home so you can ask your kids, visit the following page <https://www.netsafe.org.nz/safer- internet-day/>     [![FB & IG Proudly supporting 2](https://aws1.discourse- cdn.com/bnz/optimized/2X/d/daa286b1727bf23324a80964f2bcf67079936775_2_500x500.jpeg) FB &amp; IG Proudly supporting 21200×1200 519 KB ](https://aws1.discourse- cdn.com/bnz/original/2X/d/daa286b1727bf23324a80964f2bcf67079936775.jpeg \"FB &amp; IG Proudly supporting 2\")   ...  bnz       \n",
            "1  When I go online to set a end date on a automatic payment it will not allow a date to be entered forcing me to call to get it done.  I was told it’s a known issue but is there any ideas on when the issue will be fixed, it’s been months and months since the new internet banking was rolled out and it’s never worked on this system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  bnz       \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining airline_forum_updated\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   post_text  ...  forum_name\n",
            "0  Hey all - April credited 96 trips (~ 84 hours) of pay with 21 days off. Definitely much improved on loads this month, seeing quite a few full to the new limited capacity (~67%). It was rare to see less than 40 people per flight by the end of the month and noticed a LOT more people in the airport terminals. Hoping to see this trend continue!  MCO-HOU (DH) HOU-MAF-LAS-PHX-MCO (1st leg DH) OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF  ...  airline   \n",
            "1  It’s great to see a rebound, even if it’s small. Have you heard of any plans for Southwest to expand routes? I would think that with some of the legacies plan to reduce their routes, it might be a good time for airlines like LUv and JBLU to pick up market share strategically (without taking on too much additional variable costs).                                                                                                                                                                                                                                                                                ...  airline   \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining quickfile_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          post_text  ...  forum_name\n",
            "0  HI, Just disconnected from Santander to change to Starling and it isn’t listed?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  quickfile \n",
            "1  Hi [@daveh001](/u/daveh001) Are you trying to connect this from “Open Banking” by any chance? The Starling feed isn’t actually Open Banking (although I believe this will change in the future). As long as you see the Starling logo on your account in QuickFile, you can go to the bank statement view, select **More Options** >> **Activate Bank Feed**. If you don’t see the Starling logo, select the account you’re trying to link up, as above, go to the ban statement view, **More Options** >> **Settings** , and set it to Starling. Both the logo and the option to activate the feed should then appear for you.   ...  quickfile \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining codecombat_forum_posts\n",
            "  post_text  post_id  ...          topic_slug  forum_name\n",
            "0  88902     athian   ...  bookkeeper-help-me  codecombat\n",
            "1  88904     AnseDra  ...  bookkeeper-help-me  codecombat\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining schizophrenia_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ...     forum_name\n",
            "0  ![](http://www.peteearley.com/wp-content/themes/education/images/favicon.ico) [Pete Earley – 8 Jun 20](http://www.peteearley.com/2020/06/08/federal-govt- accused-of-abandoning-research-that-would-provide-short-term-help-to-the-most- seriously-mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29 \"12:00PM - 08 June 2020\") ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/2/f2e8cca16cf15cebbe4ee807f64c29a9dcc09d25.jpeg) ### [Federal Govt. Accused Of Abandoning Research That Would Provide Short Term...](http://www.peteearley.com/2020/06/08/federal-govt-accused-of- abandoning-research-that-would-provide-short-term-help-to-the-most-seriously- mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29) Dr. E. Fuller Torrey rips into NIMH, its advisory board and NAMI (6-8-20) Dr. E. Fuller Torrey is again accusing the National Institutes of Mental Health of virtually abandoning clinical trials that could help Americans with schizophrenia and bipolar...   ...  schizophrenia\n",
            "1  I’m taking my first capsule of Lumateperone 42 mg tonight. Steady state should be reached in 5 days. Will share my experiences here.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  schizophrenia\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining folksy_forum_posts\n",
            "   post_text       post_id  ...                     topic_slug forum_name\n",
            "0  412784     DeesDesigns   ...  when-is-fathers-day-in-the-uk  folksy   \n",
            "1  412789     leavalleyart  ...  when-is-fathers-day-in-the-uk  folksy   \n",
            "\n",
            "[2 rows x 13 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.</td>\n",
              "      <td>25677</td>\n",
              "      <td>4635</td>\n",
              "      <td>Tom</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2020-06-04T17:03:37.547Z</td>\n",
              "      <td>2020-06-04T17:04:06.818Z</td>\n",
              "      <td>14</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i tried choc cherry only one i didnt like</td>\n",
              "      <td>25678</td>\n",
              "      <td>4856</td>\n",
              "      <td>matt009</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>14</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I’ll be ordering one bag of Vanilla Black Edition next shipment - but for now I use the Vanilla boost in Chocolate Huel B. E. and it goes great together, Vanilla chocolately flavor. I also use Vanilla Almond Milk, so that might help the Vanilla flavor too.</td>\n",
              "      <td>25680</td>\n",
              "      <td>4859</td>\n",
              "      <td>Justin_Keikhlasan</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-04T20:01:55.915Z</td>\n",
              "      <td>2020-06-04T20:01:55.915Z</td>\n",
              "      <td>13</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>My next order is bringing mint chocolate, I’m pretty excited to try it!</td>\n",
              "      <td>25683</td>\n",
              "      <td>2758</td>\n",
              "      <td>DM87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-04T22:18:10.521Z</td>\n",
              "      <td>2020-06-04T22:18:10.521Z</td>\n",
              "      <td>13</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Thanks. How much booster do you need to use, or does it vary?</td>\n",
              "      <td>25729</td>\n",
              "      <td>34</td>\n",
              "      <td>Desert_Way</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>11</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2020-06-07T14:32:13.890Z</td>\n",
              "      <td>2020-06-07T14:32:13.890Z</td>\n",
              "      <td>9</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ... forum_name\n",
              "2    Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.   ...  huel     \n",
              "3   i tried choc cherry only one i didnt like                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  huel     \n",
              "5    I’ll be ordering one bag of Vanilla Black Edition next shipment - but for now I use the Vanilla boost in Chocolate Huel B. E. and it goes great together, Vanilla chocolately flavor. I also use Vanilla Almond Milk, so that might help the Vanilla flavor too.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  huel     \n",
              "6    My next order is bringing mint chocolate, I’m pretty excited to try it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...  huel     \n",
              "10   Thanks. How much booster do you need to use, or does it vary?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...  huel     \n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ngBhDz0Jpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "6f45a8cf-965f-4001-90cb-3460bd40eba7"
      },
      "source": [
        "# Shuffle df\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42162</th>\n",
              "      <td>That’s great, thanks for letting me know!</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32745</th>\n",
              "      <td>26562</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42529</th>\n",
              "      <td>Tai, The tuition increase is due to increased operational costs that ATP is experiencing. These include increased av-gas prices, an extra ten hours of multi engine time being included in the program and as you mentioned, the training bundle being included in the price. Chris</td>\n",
              "      <td>airline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85564</th>\n",
              "      <td>You don’t need the original password as an Admin to change a client password. ![](https://community.quickfile.co.uk/uploads/default/original/2X/d/d5cd1480aa9af1d61e0d39a4f62c95572dcf9505.png) ![](https://community.quickfile.co.uk/uploads/default/original/2X/1/1f3b0a9ff987d3dcdd86ae2cf3089bdbbf12465e.png) Also clients can reset their passwords from `https://myarea.quickfile.co.uk`. In general clients rarely need to know their login credentials as whenever you send them an invoice or statement they will be able to get straight it from the link.</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90465</th>\n",
              "      <td>9773</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  post_text  forum_name\n",
              "42162  That’s great, thanks for letting me know!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             quickfile \n",
              "32745  26562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 codecombat\n",
              "42529  Tai, The tuition increase is due to increased operational costs that ATP is experiencing. These include increased av-gas prices, an extra ten hours of multi engine time being included in the program and as you mentioned, the training bundle being included in the price. Chris                                                                                                                                                                                                                                                                                   airline   \n",
              "85564  You don’t need the original password as an Admin to change a client password. ![](https://community.quickfile.co.uk/uploads/default/original/2X/d/d5cd1480aa9af1d61e0d39a4f62c95572dcf9505.png) ![](https://community.quickfile.co.uk/uploads/default/original/2X/1/1f3b0a9ff987d3dcdd86ae2cf3089bdbbf12465e.png) Also clients can reset their passwords from `https://myarea.quickfile.co.uk`. In general clients rarely need to know their login credentials as whenever you send them an invoice or statement they will be able to get straight it from the link.  quickfile \n",
              "90465  9773                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  codecombat"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4SZ-DOOEbt9",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Data Cleansing and Prep\n",
        "Now we've loaded the data, we must remove noise from the dataset. Please explore some techniques in which we could clean the data in order to us to see how well pre-trained BERT works on our dataset. Luckily, due to the way BERT tokenises the data, we don't need to the same extent of data preprocessing as required of previous NLP models. However we still need to -\n",
        "\n",
        "1. Filter nulls\n",
        "2. Filter for duplicates\n",
        "3. [Optional] Remove post_text which does not have vocab in pre-trained BERT. 4. Later, we will leave this in for finetuning.\n",
        "5. Hyperlinks\n",
        "6. Foreign languages - there are multilingual BERT models\n",
        "\n",
        "Any more you can think of?\n",
        "Encode the labels - map categorical labels to numerical values\n",
        "See here for Pandas cleaning tutorials\n",
        "See here for beginner EDA tutorial for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxcETy6tEevJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "d204bd5b-bf1f-43c5-d8fe-9086f8df066f"
      },
      "source": [
        "df_all.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_text            object \n",
              "post_id              object \n",
              "user_id              object \n",
              "username             object \n",
              "reply_to_post_num    float64\n",
              "topic_id             object \n",
              "post_num             int64  \n",
              "reply_count          float64\n",
              "created_at           object \n",
              "updated_at           object \n",
              "num_reads            int64  \n",
              "topic_slug           object \n",
              "forum_name           object \n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKt45gM9N--n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "168928bc-42c9-4a31-b2a8-e1ff6f5f8c46"
      },
      "source": [
        "df_all.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_text            0\n",
              "post_id              0\n",
              "user_id              0\n",
              "username             0\n",
              "reply_to_post_num    0\n",
              "topic_id             0\n",
              "post_num             0\n",
              "reply_count          0\n",
              "created_at           0\n",
              "updated_at           0\n",
              "num_reads            0\n",
              "topic_slug           0\n",
              "forum_name           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFyhjI-Bj4jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "df_all.drop_duplicates\n",
        "df_all['post_text'] = df_all['post_text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtfguvdjI5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b9a80e7-9e8a-44a1-b61d-f96728081498"
      },
      "source": [
        "df_all.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97621, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWGrqJtusCSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "b6cdee45-61fc-4cab-be6f-52a1713cebb7"
      },
      "source": [
        "# Shuffle df\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18227</th>\n",
              "      <td>Adam, I appreciate the info.  I thought I read you had a family, but when I went back to reread some of your background, I could not find specific information about moving a family. Thanks again!</td>\n",
              "      <td>airline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66723</th>\n",
              "      <td>a heart breaking and moving movie the only disney film I like … about a couple who grow old together but he eventually loses her ![:frowning:](</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53563</th>\n",
              "      <td>27532</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31352</th>\n",
              "      <td>[@Glenn](/u/glenn) Awesome great to hear that—and feel free to get in touch anytime!</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26340</th>\n",
              "      <td>236058</td>\n",
              "      <td>folksy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                 post_text     forum_name\n",
              "18227  Adam, I appreciate the info.  I thought I read you had a family, but when I went back to reread some of your background, I could not find specific information about moving a family. Thanks again!  airline      \n",
              "66723  a heart breaking and moving movie the only disney film I like … about a couple who grow old together but he eventually loses her ![:frowning:](                                                      schizophrenia\n",
              "53563  27532                                                                                                                                                                                                codecombat   \n",
              "31352  [@Glenn](/u/glenn) Awesome great to hear that—and feel free to get in touch anytime!                                                                                                                 quickfile    \n",
              "26340  236058                                                                                                                                                                                               folksy       "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvwKwRYNsOz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3ded650-aff1-4668-da6d-ce1d0741f4e8"
      },
      "source": [
        "df_all['forum_name'] = df_all['forum_name'].astype('category')\n",
        "df_all['forum_name_encoded'] = df_all['forum_name'].cat.codes.astype('int32')\n",
        "df_all.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91258</th>\n",
              "      <td>Anthony, Thirty five is certainly plenty young enough to be a pilot, take a look in the FAQ section as Adam gives it a good break down of what you can expect from the career at various ages. I would say that more than anything, you need to go take an introductory flight and see if flying is really the thing for you. Chris</td>\n",
              "      <td>28996</td>\n",
              "      <td>3</td>\n",
              "      <td>Chris</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11442</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2018-01-21T01:58:59.000Z</td>\n",
              "      <td>2018-01-21T01:59:02.732Z</td>\n",
              "      <td>64</td>\n",
              "      <td>career-change-from-it-engineer-to-pilot</td>\n",
              "      <td>airline</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97123</th>\n",
              "      <td>58264</td>\n",
              "      <td>xantar</td>\n",
              "      <td>2018-08-15T09:41:22.399Z</td>\n",
              "      <td>If this is not the place where I post ideas, tell me in the comments.I think codecombat needs a mage update. You are doing great work, but:Firstly, more spells. How about making holy magic with spells like blessing (take 0.5 less damage for some time)? Also renaming Life Magic to Nature Magic would be great, moving Regen to holy magic, and replacing it with Root spell causing poisoning, dealing damage and freezing nearby enemies for some time. I think that it would be possible to like mana limit, for example 6, so you can have one V tier magic tome and one I tier magic tome. Ah, and add blink please (a kind of teleportation, doesn’t pass through walls)New weapons: new staffs with new abilities. And make it like mage is a class that needs to balance so sometimes there wasn’t a “better” staff, just like one staff is good combined with bla bla bla magic and the second with ble ble ble magic, or one is more defensive and one more offensive. Or one would have spells that actually need a good code tactic and the second was easy to play, but softly weaker. This would motivate players a lot! The other thing is adding new potions to omar- he is the brewmaster!Armor rework: armors should give you special bonuses, for example the gryffin armor would give you speed. I’m waiting for new mage armors too- there are only 10 robes and 8 wizard hats (not including the viking helmet and the fur cap)So, I am waiting to see any reaction from you, users of codecombat, to my idea and you, the creators.Yours Sincerely,The 21st Human Team player in Dueling Grounds</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2018-08-15T20:47:26.896Z</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "      <td>15584</td>\n",
              "      <td>8927</td>\n",
              "      <td>idea-mage-items</td>\n",
              "      <td>codecombat</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>822</th>\n",
              "      <td>19935</td>\n",
              "      <td>nick</td>\n",
              "      <td>2015-12-20T14:29:01.591Z</td>\n",
              "      <td>Got it, sorry about that–looks like we added Nalfar’s soul-link spell finally but forgot to publish the Component that provides its logic.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2015-12-20T14:29:01.591Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>6160</td>\n",
              "      <td>1</td>\n",
              "      <td>kelvintaph-defiler-error</td>\n",
              "      <td>codecombat</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50974</th>\n",
              "      <td>25072</td>\n",
              "      <td>CryogenicMiner</td>\n",
              "      <td>2016-06-07T20:25:15.951Z</td>\n",
              "      <td>Exactly! nice quick work guys.</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2016-06-07T20:25:15.951Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "      <td>7883</td>\n",
              "      <td>4486</td>\n",
              "      <td>no-link-to-campain</td>\n",
              "      <td>codecombat</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87627</th>\n",
              "      <td>9651</td>\n",
              "      <td>Feinty</td>\n",
              "      <td>2015-03-13T00:50:25.286Z</td>\n",
              "      <td>I made that change and a handful of others but my code has an infinity loop could someone find the problem and tell me how to fix it  //Commented out to stop infinite loop.// Use object literals to walk the safe path and collect the gems.// You cannot use moveXY() on this level! Use move() to get around.gems = this.findItems();while (this.pos.x &amp;lt; 20) {\\t// move() takes objects with x and y properties, not just numbers.\\tthis.move({x: 20, y: 35});}while (this.pos.x &amp;lt; 25) {\\t// A gem's position is an object with x and y properties.\\tgem0 = gems[0];\\tthis.move(gem[0].pos);}while (this.pos.x &amp;lt; 30 ){    this.move({x:30,y:35});    }while(this.pos.x &amp;lt; 35){    this.move(gems[1].pos);    }// While your x is less than 30,// Use an object to move to 30, 35.// While your x is less than 35,// Move to the position of gems[1].// Get to the last couple of gems yourself!</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2015-03-13T00:50:25.286Z</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>56</td>\n",
              "      <td>3051</td>\n",
              "      <td>1486</td>\n",
              "      <td>slalom-help-with-code</td>\n",
              "      <td>codecombat</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                 post_text  ... forum_name_encoded\n",
              "91258  Anthony, Thirty five is certainly plenty young enough to be a pilot, take a look in the FAQ section as Adam gives it a good break down of what you can expect from the career at various ages. I would say that more than anything, you need to go take an introductory flight and see if flying is really the thing for you. Chris  ...  0                \n",
              "97123  58264                                                                                                                                                                                                                                                                                                                                ...  2                \n",
              "822    19935                                                                                                                                                                                                                                                                                                                                ...  2                \n",
              "50974  25072                                                                                                                                                                                                                                                                                                                                ...  2                \n",
              "87627  9651                                                                                                                                                                                                                                                                                                                                 ...  2                \n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_cs4evqsYUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "20cba7ef-dc00-4141-8b16-3a27c8f94e4b"
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting html2text\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1nz8GJZsc0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "b5595898-c87f-4a23-a6c9-117c0c89b3e4"
      },
      "source": [
        "import html2text\n",
        "h = html2text.HTML2Text()\n",
        "h.unicode_snob = True\n",
        "df_all['post_text'] = df_all['post_text'].apply((lambda x: h.handle(x).replace('\\n', ' ').replace('  ', ' ')))\n",
        "df_all[df_all['forum_name']=='folksy'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30354</th>\n",
              "      <td>237416</td>\n",
              "      <td>LittleTrinketGirl</td>\n",
              "      <td>2017-02-25T17:03:02.840Z</td>\n",
              "      <td>Ive asked Eshopsuk and crafturday to retweet for you Crafturday have done it already Yay x</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2017-02-25T17:03:02.840Z</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>44</td>\n",
              "      <td>13419</td>\n",
              "      <td>5730</td>\n",
              "      <td>just-joined-twitter</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78396</th>\n",
              "      <td>79814</td>\n",
              "      <td>Amberlilly</td>\n",
              "      <td>2015-03-06T08:31:40.067Z</td>\n",
              "      <td>Oh thxs a class mention href u paulinescrafts PaulinesCrafts a wouldve thought there was a way, unless I log out, but yes, I will ask them x</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2015-03-06T08:31:40.067Z</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>68</td>\n",
              "      <td>5916</td>\n",
              "      <td>1743</td>\n",
              "      <td>how-to-empty-basket</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43649</th>\n",
              "      <td>196976</td>\n",
              "      <td>atalantaart</td>\n",
              "      <td>2016-07-18T22:23:32.250Z</td>\n",
              "      <td>Congratulations,someone liked what you do,I am sure there will be others</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2016-07-18T22:23:32.250Z</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>70</td>\n",
              "      <td>11924</td>\n",
              "      <td>4973</td>\n",
              "      <td>yayyyyyyyy-first-sale-high</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82698</th>\n",
              "      <td>278738</td>\n",
              "      <td>jessicagoulty</td>\n",
              "      <td>2017-11-27T08:48:47.115Z</td>\n",
              "      <td>They do through other venues, not so on Folksysigh</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2017-11-27T08:48:47.115Z</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>40</td>\n",
              "      <td>15025</td>\n",
              "      <td>6462</td>\n",
              "      <td>showcase-your-work-with-collages</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46410</th>\n",
              "      <td>384362</td>\n",
              "      <td>heatherandhome</td>\n",
              "      <td>2019-11-21T12:41:36.947Z</td>\n",
              "      <td>Do you mean under the t amp cs of my Folksy shop</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2019-11-21T12:41:36.947Z</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>28</td>\n",
              "      <td>19074</td>\n",
              "      <td>8408</td>\n",
              "      <td>product-liability-insurance-query</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      post_text            post_id  ... forum_name forum_name_encoded\n",
              "30354  237416    LittleTrinketGirl  ...  folksy     3                \n",
              "78396  79814     Amberlilly         ...  folksy     3                \n",
              "43649  196976    atalantaart        ...  folksy     3                \n",
              "82698  278738    jessicagoulty      ...  folksy     3                \n",
              "46410  384362    heatherandhome     ...  folksy     3                \n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Cl93gyrv04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df_all.post_text.values\n",
        "labels = df_all.forum_name_encoded.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdJy3Ar8stpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "b6ea00631575417bb3dd4736da1455d8",
            "4b4ee0ffb5e4417baaa226f922e60206",
            "3b7593d3930247119675d55f4fa2084c",
            "ddf32773b8494b6199929c1c481737a4",
            "619c3a115c7f4ea984e4be928d3a2941",
            "427c0bd83bd54e7b8b9d7df3f5333bba",
            "f6bd52752ab24003bee2696ae12cf1d2",
            "2730b9bb538a41daa9b675640ea61d28"
          ]
        },
        "outputId": "a1fc9c2d-2398-44f2-ec84-3479fea1ddd2"
      },
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Load pretrained DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6ea00631575417bb3dd4736da1455d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36084</th>\n",
              "      <td>87941</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61318</th>\n",
              "      <td>I was supposed to take trihexephenidyl, but all those anti-tremor medication gives me really bad dry mouth and gives my like panic attack confusion. So I can’t use them</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17020</th>\n",
              "      <td>Well great Never heard it before Will check out later</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34443</th>\n",
              "      <td>Glad to hear this is working. Of course, if you have any further issues, please let us know.</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66707</th>\n",
              "      <td>25604</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                       post_text     forum_name\n",
              "36084  87941                                                                                                                                                                      codecombat   \n",
              "61318  I was supposed to take trihexephenidyl, but all those anti-tremor medication gives me really bad dry mouth and gives my like panic attack confusion. So I can’t use them   schizophrenia\n",
              "17020  Well great Never heard it before Will check out later                                                                                                                      schizophrenia\n",
              "34443  Glad to hear this is working. Of course, if you have any further issues, please let us know.                                                                               quickfile    \n",
              "66707  25604                                                                                                                                                                      codecombat   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xnWfTgq_iQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0419c406-f112-4f6a-a67f-5e876f5602fc"
      },
      "source": [
        "max_len = 0\n",
        "# For first 10 sentences - \n",
        "for s in sentences[:10]:\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(s, add_special_tokens=True)\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxsuvVeOs-F3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6965683e-4c25-4cdb-bdb6-a26876131620"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for s in sentences:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        s,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Okay. If you’re state-side, call your local FSDO to verify, BUT I’m pretty sure I have it figured out based on what I remember. You have two options: you can get your commercial license with a foreign restriction. This does not require you to take a written. The ride will essentially be the equivalent of a BFR. (This is the part I am not so confident about. Doing a BFR may only get you a PPL with a foreign restriction. This is why you need to talk to the FSDO.) you can get your commercial license with your foreign restriction removed. This requires you to meet ALL of the FAA requirements. You’ll actually have to take a private check ride first, and then a commercial check ride, which means that you will have to take the PAR and CAX written. If you want to be able to fly commercially in IMC, or more than 50NM or at night, you will also have to get your Instrument rating too. No matter what you decide to do you need to compare your logbook with the requirements listed in the FARs. Each country has different standards. It is likely that your experience will not meet FAA standards. If your logbook doesn’t meet FAA standards, go do what’s required before scheduling your ride. Again, the FSDO should be your main source of info. These kinds of situations come up so rarely it’s hard for me to remember all of the details. Tory \n",
            "Token IDs: tensor([  101,  3100,  1012,  2065,  2017,  1521,  2128,  2110,  1011,  2217,\n",
            "         1010,  2655,  2115,  2334,  1042, 16150,  2080,  2000, 20410,  1010,\n",
            "         2021,  1045,  1521,  1049,  3492,  2469,  1045,  2031,  2009,  6618,\n",
            "         2041,  2241,  2006,  2054,  1045,  3342,  1012,  2017,  2031,  2048,\n",
            "         7047,  1024,  2017,  2064,  2131,  2115,  3293,  6105,  2007,  1037,\n",
            "         3097, 16840,  1012,  2023,  2515,  2025,  5478,  2017,  2000,  2202,\n",
            "         1037,  2517,  1012,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVwh2JN7taj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "503a059c-4b57-490d-b4f2-e04092b6d2b0"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f'{train_size:>5,} training samples')\n",
        "print(f'{val_size:>5,} validation samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87,858 training samples\n",
            "9,763 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wI4Oa4thAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4LDrYSZtmm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a20c954b2ef34262be942e0fb9420073",
            "99d2a18b397b461994c5ac68aa10f956",
            "6d44034f4a574540888422bc1939f3c0",
            "e4983d34d5f948119b5905c015595027",
            "b652646ab7b547fd9c2a5a9d244bbe12",
            "21d894673baa43259d1ea375f2a6d961",
            "d00f704033284be786a20d8072fd226e",
            "b8dcba18b75c48c7be61db92c76170a3",
            "27b57bde65d34b509b95e913bf22c7ef",
            "a9d9b60d3a784f41b378c8bd15fd7401",
            "bbc791cb8e3d4f64ab4b4bf30b61f9ca",
            "4b8819c62cd2443ca1d0568e38540e85",
            "dd88a06580914e2e8b96c350f679e975",
            "45b415c577d44eb0998131b1f4cb636d",
            "18a8e958a8ba4cf5a08bc353d9ce5491",
            "c294e14a64824565b089354f85648d56"
          ]
        },
        "outputId": "c993c2a3-eecd-4c47-ac6a-af9f550ea801"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 7, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU. Make sure you enable the runtime clicking [Runtime]->[Change Runtime Type]->[Hardware Accelerator]->GPU->[Save]\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a20c954b2ef34262be942e0fb9420073",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27b57bde65d34b509b95e913bf22c7ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "129Kh1Xxp-d-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRM4oKGot4Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MuHxuHDuBRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wz-Ipq5u9Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGPWoVbFuC-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhMijgO1uI3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cb09b14-5669-4686-ce59-59969334494e"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(cuda)\n",
        "        b_input_mask = batch[1].to(cuda)\n",
        "        b_labels = batch[2].to(cuda)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(cuda)\n",
        "        b_input_mask = batch[1].to(cuda)\n",
        "        b_labels = batch[2].to(cuda)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:09.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:17.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:25.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:00:33.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:00:41.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:00:49.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:00:57.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:01:06.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:01:14.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:01:22.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:01:30.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:01:38.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:01:46.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:01:55.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:02:03.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:02:11.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:02:19.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:02:27.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:02:35.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:02:43.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:02:51.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:03:00.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:03:08.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:03:16.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:03:24.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:03:32.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:03:40.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:03:48.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:03:56.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:04:05.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:04:13.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:04:21.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:04:29.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:04:37.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:04:45.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:04:53.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:05:02.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:05:10.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:05:18.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:05:26.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:05:34.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:05:42.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:05:50.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:05:58.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:06:06.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:06:15.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:06:23.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:06:31.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:06:39.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:06:47.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:06:55.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:07:03.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:07:11.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:07:19.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:07:28.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:07:36.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:07:44.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:07:52.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:08:00.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:08:08.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:08:16.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:08:24.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:08:32.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:08:40.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:08:49.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:08:57.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:09:05.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:09:13.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:09:18\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation Loss: 0.15\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:08.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:16.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:24.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:00:33.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:00:41.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:00:49.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:00:57.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:01:05.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:01:13.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:01:21.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:01:29.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:01:37.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:01:53.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:02:01.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:02:10.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:02:18.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:02:26.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:02:34.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:02:42.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:02:50.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:02:58.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:03:06.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:03:14.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:03:22.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:03:30.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:03:39.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:03:47.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:03:55.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:04:03.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:04:11.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:04:19.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:04:27.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:04:35.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:04:43.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:04:51.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:04:59.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:05:07.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:05:16.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:05:24.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:05:32.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:05:40.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:05:48.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:05:56.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:06:04.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:06:12.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:06:20.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:06:28.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:06:36.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:06:45.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:06:53.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:07:01.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:07:09.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:07:17.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:07:25.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:07:33.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:07:41.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:07:49.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:07:57.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:08:05.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:08:14.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:08:22.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:08:30.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:08:38.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:08:46.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:08:54.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:09:02.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:09:10.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:09:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.15\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:08.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:16.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:24.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:00:32.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:00:40.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:00:48.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:00:56.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:01:05.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:01:13.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:01:21.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:01:29.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:01:37.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:01:53.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:02:01.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:02:09.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:02:17.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:02:25.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:02:33.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:02:42.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:02:50.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:02:58.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:03:06.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:03:14.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:03:22.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:03:30.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:03:38.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:03:46.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:03:54.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:04:02.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:04:10.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:04:18.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:04:27.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:04:35.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:04:43.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:04:51.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:04:59.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:05:07.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:05:15.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:05:23.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:05:31.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:05:39.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:05:47.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:05:55.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:06:03.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:06:11.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:06:20.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:06:28.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:06:36.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:06:44.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:06:52.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:07:00.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:07:08.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:07:16.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:07:24.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:07:32.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:07:40.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:07:48.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:07:56.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:08:04.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:08:12.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:08:21.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:08:29.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:08:37.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:08:45.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:08:53.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:09:01.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:09:09.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:09:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.16\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:08.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:16.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:24.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:00:32.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:00:40.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:00:48.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:00:56.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:01:04.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:01:12.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:01:21.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:01:29.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:01:37.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:01:53.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:02:01.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:02:09.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:02:17.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:02:25.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:02:33.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:02:41.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:02:49.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:02:57.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:03:05.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:03:13.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:03:21.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:03:30.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:03:38.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:03:46.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:03:54.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:04:02.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:04:10.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:04:18.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:04:26.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:04:34.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:04:42.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:04:50.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:04:58.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:05:06.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:05:14.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:05:22.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:05:30.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:05:38.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:05:46.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:05:54.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:06:03.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:06:11.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:06:19.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:06:27.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:06:35.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:06:43.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:06:51.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:06:59.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:07:07.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:07:15.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:07:23.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:07:31.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:07:39.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:07:47.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:07:55.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:08:03.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:08:11.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:08:19.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:08:27.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:08:36.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:08:44.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:08:52.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:09:00.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:09:08.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:09:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.17\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:38:12 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VhXEhylkSgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2decb6b0-024a-4529-b829-1eca3c3d5265"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0:09:18</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:09:15</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:09:14</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:09:13</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1      0.21           0.15         0.95           0:09:18       0:00:18       \n",
              "2      0.11           0.15         0.96           0:09:15       0:00:18       \n",
              "3      0.07           0.16         0.96           0:09:14       0:00:18       \n",
              "4      0.06           0.17         0.96           0:09:13       0:00:18       "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPDUcv0kkWy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "daa61413-a655-42c9-8316-e20a2e498507"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}